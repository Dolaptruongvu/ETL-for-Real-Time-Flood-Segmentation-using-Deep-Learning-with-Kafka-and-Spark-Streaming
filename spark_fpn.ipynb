{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import logging\n",
    "from kafka import KafkaProducer\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, struct, udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "import segmentation_models_pytorch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pymongo import MongoClient\n",
    "import gridfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging (Driver-Side)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "python_version = 'D:/Anaconda/envs/cuda/python.exe'\n",
    "os.environ['PYSPARK_PYTHON'] = python_version\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Spark session on driver.\n"
     ]
    }
   ],
   "source": [
    "# Kafka and Spark configuration (Driver-Side)\n",
    "kafka_server = 'localhost:9092'\n",
    "topic_name = 'RandomImage'\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}', \n",
    "    'org.apache.kafka:kafka-clients:3.7.0',\n",
    "    'org.mongodb.spark:mongo-spark-connector_2.12:2.4.1'  # Use an older, stable version\n",
    "]\n",
    "\n",
    "# Spark session setup (Driver-Side)\n",
    "logger.info(\"Initializing Spark session on driver.\")\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName(\"kafka-example\") \\\n",
    "                    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.executor.cores\", \"8\") \\\n",
    "                    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model and transformation setup (Driver-Side)\n",
    "\n",
    "MODEL_LOAD_PATH = './models/supreme/abhi_sudo_full_2-pretrained_preproc_1-FPN+-ep_9-0.01-24.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FPN ####\n",
    "ENCODER_DEPTH=5\n",
    "DPC=256\n",
    "DSC = 128\n",
    "BATCH_SIZE = 16\n",
    "LR = [1e-2]\n",
    "EPOCHS= 50\n",
    "ENCODER_NAME= 'resnet34'\n",
    "\n",
    "\n",
    "CLASSES={'Background':0,'Building-flooded':1,'Building-non-flooded':2,'Road-flooded':3,'Road-non-flooded':4,\n",
    "         'Water':5,'Tree':6,'Vehicle':7,'Pool':8,'Grass':9}\n",
    "IMG_DIM= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): FPNDecoder(\n",
       "    (p5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p4): FPNBlock(\n",
       "      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNBlock(\n",
       "      (skip_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNBlock(\n",
       "      (skip_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (seg_blocks): ModuleList(\n",
       "      (0): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeBlock()\n",
       "    (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(128, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = segmentation_models_pytorch.FPN(encoder_name=ENCODER_NAME, encoder_depth=ENCODER_DEPTH,\n",
    "                                              decoder_pyramid_channels=DPC, decoder_segmentation_channels=DSC, classes=len(CLASSES))\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL_LOAD_PATH,map_location=torch.device('cuda')))\n",
    "# model = model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Broadcast the model and transformation needed on the workers (Driver-Side)\n",
    "broadcast_model = spark.sparkContext.broadcast(model)\n",
    "broadcast_val_transform = spark.sparkContext.broadcast(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Functions for image transformation and mask processing (Worker-Side)\n",
    "def reverse_transform_mask(inp):\n",
    "    inp = inp.transpose((1, 2, 0))\n",
    "    t_mask = np.argmax(inp, axis=2).astype('float32')\n",
    "    t_mask = cv2.resize(t_mask, dsize=(4000, 3000))\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    t_mask = cv2.erode(t_mask, kernel, iterations=1)\n",
    "    return t_mask\n",
    "\n",
    "# Pandas UDF to transform images (Worker-Side)\n",
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR)\n",
    "def transform_image_pandas_udf(image_bytes_series: pd.Series) -> pd.Series:\n",
    "    logger.info(\"Starting image transformation with Pandas UDF\")\n",
    "    def transform_image_single(image_bytes):\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((IMG_DIM, IMG_DIM))\n",
    "        img = np.array(img)\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        img = cv2.bilateralFilter(img, 5, 75, 75)\n",
    "        img = cv2.erode(cv2.dilate(img, kernel, iterations=2), kernel, iterations=1)\n",
    "        transformed_img = broadcast_val_transform.value(img)\n",
    "        return transformed_img.numpy().flatten().astype(np.float32)\n",
    "    \n",
    "    return image_bytes_series.apply(transform_image_single)\n",
    "\n",
    "# Predict function for batch processing (Worker-Side)\n",
    "def predict_batch_fn():\n",
    "    model = broadcast_model.value\n",
    "    model.eval()\n",
    "    def predict(inputs: np.ndarray):\n",
    "        inputs = inputs.reshape(-1, 3, IMG_DIM, IMG_DIM)\n",
    "        inputs_tensor = torch.from_numpy(inputs).to(device).float()\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs_tensor)\n",
    "            preds = torch.sigmoid(preds).cpu().numpy()\n",
    "        return preds.reshape(inputs.shape[0], -1)\n",
    "    return predict\n",
    "\n",
    "image_predict_udf = predict_batch_udf(\n",
    "    predict_batch_fn,\n",
    "    return_type=ArrayType(FloatType()),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_tensor_shapes=[[3 * IMG_DIM * IMG_DIM]]\n",
    ")\n",
    "\n",
    "# UDF for transforming the mask and saving to MongoDB (Worker-Side)\n",
    "def save_to_mongo(key, predictions):\n",
    "    logger.info(f\"Saving to MongoDB for key: {key}\")\n",
    "    predictions = np.array(predictions).reshape(len(CLASSES), IMG_DIM, IMG_DIM)\n",
    "    f_mask = reverse_transform_mask(predictions)\n",
    "    \n",
    "    # Convert mask to bytes and save to GridFS (Worker-Side)\n",
    "    mask_bytes = io.BytesIO()\n",
    "    np.save(mask_bytes, f_mask)\n",
    "    mask_bytes.seek(0)\n",
    "\n",
    "    # Setup MongoDB connection inside the worker function (Worker-Side)\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['image_db']\n",
    "    fs = gridfs.GridFS(db)\n",
    "    collection = db['image_masks']\n",
    "\n",
    "    mask_id = fs.put(mask_bytes, filename=f\"{key}_mask.npy\")\n",
    "\n",
    "    # Save to MongoDB (Worker-Side)\n",
    "    image_doc = {\n",
    "        \"image_id\": key,\n",
    "        \"mask_gridfs_id\": mask_id\n",
    "    }\n",
    "    collection.insert_one(image_doc)\n",
    "\n",
    "    logger.info(f\"Successfully saved to MongoDB for key: {key}\")\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_to_mongo_udf = udf(save_to_mongo, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Batch processing function (Driver-Side)\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    logger.info(f\"Foreach Batch Function called on Epoch ID: {epoch_id}.\")\n",
    "    if df.count() > 0:\n",
    "        logger.info(f\"Processing {df.count()} records in the batch.\")\n",
    "\n",
    "        # Transform the image (Worker-Side)\n",
    "        df = df.withColumn(\"transformed_image_bytes\", transform_image_pandas_udf(col(\"image_bytes\")))\n",
    "\n",
    "        # Make predictions (Worker-Side)\n",
    "        df = df.withColumn(\"predictions\", image_predict_udf(\"transformed_image_bytes\"))\n",
    "\n",
    "        # Save predictions to MongoDB (Worker-Side)\n",
    "        df = df.withColumn(\"saved_to_mongo\", save_to_mongo_udf(col(\"key\"), col(\"predictions\")))\n",
    "\n",
    "        df.show()\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Empty DataFrame received in foreachBatch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean the checkpoint directory (Driver-Side)\n",
    "import shutil\n",
    "shutil.rmtree(\"checkpoint_dir\", ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting up Kafka stream on driver.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame from Kafka stream (Driver-Side)\n",
    "logger.info(\"Setting up Kafka stream on driver.\")\n",
    "streamRawDf = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "streamDF = streamRawDf.select(col(\"key\").cast(\"string\"), col(\"value\").alias(\"image_bytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting the streaming query on driver.\n",
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 56009)\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 0.\n",
      "INFO:__main__:Empty DataFrame received in foreachBatch\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 1.\n",
      "INFO:__main__:Processing 1 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|  key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|10163|[FF D8 FF E0 00 1...|   [-0.2170563, -0.2...|[0.009559345, 0.0...|         10163|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 2.\n",
      "INFO:__main__:Processing 2 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|  key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|10164|[FF D8 FF E0 00 1...|   [-1.6384109, -1.6...|[0.008425804, 0.0...|         10164|\n",
      "|10167|[FF D8 FF E0 00 1...|   [-0.6280504, -0.6...|[0.015789637, 0.0...|         10167|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 3.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|  key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|10174|[FF D8 FF E0 00 1...|   [0.3651854, 0.365...|[0.06484638, 0.06...|         10174|\n",
      "|10183|[FF D8 FF E0 00 1...|   [-1.004795, -1.00...|[0.028954849, 0.0...|         10183|\n",
      "|10808|[FF D8 FF E0 00 1...|   [0.056939743, 0.0...|[0.3601722, 0.362...|         10808|\n",
      "|10812|[FF D8 FF E0 00 1...|   [-0.06293353, -0....|[0.040886488, 0.0...|         10812|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 4.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|  key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|10813|[FF D8 FF E0 00 1...|   [-0.2170563, -0.2...|[0.050122224, 0.0...|         10813|\n",
      "|10814|[FF D8 FF E0 00 1...|   [0.005565486, 0.0...|[0.07253498, 0.07...|         10814|\n",
      "|10823|[FF D8 FF E0 00 1...|   [0.70768046, 0.70...|[0.3259645, 0.340...|         10823|\n",
      "|10829|[FF D8 FF E0 00 1...|   [-0.02868402, -0....|[0.0800472, 0.068...|         10829|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 5.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|  key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "|10838|[FF D8 FF E0 00 1...|   [-0.7307989, -0.7...|[0.010882005, 0.0...|         10838|\n",
      "|10839|[FF D8 FF E0 00 1...|   [1.1357993, 1.135...|[0.11213306, 0.10...|         10839|\n",
      "|10843|[FF D8 FF E0 00 1...|   [-1.4157891, -1.4...|[0.09359431, 0.08...|         10843|\n",
      "|11483|[FF D8 FF E0 00 1...|   [-0.7650484, -0.7...|[0.014905993, 0.0...|         11483|\n",
      "+-----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 6.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|6336|[FF D8 FF E0 00 1...|   [-0.6280504, -0.6...|[0.028736291, 0.0...|          6336|\n",
      "|6342|[FF D8 FF E0 00 1...|   [-0.49105233, -0....|[0.007755368, 0.0...|          6342|\n",
      "|6353|[FF D8 FF E0 00 1...|   [-0.06293353, -0....|[0.034245737, 0.0...|          6353|\n",
      "|6362|[FF D8 FF E0 00 1...|   [-0.11430778, -0....|[0.009015868, 0.0...|          6362|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 7.\n",
      "INFO:__main__:Processing 5 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|6371|[FF D8 FF E0 00 1...|   [-0.6451751, -0.6...|[0.003623267, 0.0...|          6371|\n",
      "|6377|[FF D8 FF E0 00 1...|   [-1.4671633, -1.4...|[0.014243345, 0.0...|          6377|\n",
      "|6383|[FF D8 FF E0 00 1...|   [-0.19993155, -0....|[0.08016974, 0.07...|          6383|\n",
      "|6389|[FF D8 FF E0 00 1...|   [1.8721637, 1.872...|[0.27442318, 0.27...|          6389|\n",
      "|6391|[FF D8 FF E0 00 1...|   [0.24531215, 0.24...|[0.03758206, 0.02...|          6391|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 8.\n",
      "INFO:__main__:Processing 5 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|6394|[FF D8 FF E0 00 1...|   [0.810429, 0.8104...|[0.03447219, 0.02...|          6394|\n",
      "|6405|[FF D8 FF E0 00 1...|   [0.34806067, 0.34...|[0.047654156, 0.0...|          6405|\n",
      "|6412|[FF D8 FF E0 00 1...|   [0.41655967, 0.41...|[0.36946324, 0.37...|          6412|\n",
      "|6417|[FF D8 FF E0 00 1...|   [0.6220567, 0.622...|[0.7750926, 0.824...|          6417|\n",
      "|6419|[FF D8 FF E0 00 1...|   [0.63918144, 0.63...|[0.8060939, 0.849...|          6419|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 9.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start the streaming query (Driver-Side)\n",
    "logger.info(\"Starting the streaming query on driver.\")\n",
    "query = streamDF.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query (Driver-Side)\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
