{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75034179-735c-4fc3-a2ac-7f1e35e7f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from time import sleep\n",
    "import threading\n",
    "import os\n",
    "import io\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from kafka import KafkaProducer\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch\n",
    "from pymongo import MongoClient\n",
    "import gridfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b71dc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Configuration\n",
    "mongo_client = MongoClient('localhost', 27017)\n",
    "db = mongo_client['bigdata']\n",
    "collection = db['processed_images']\n",
    "fs = gridfs.GridFS(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b559340-dc6e-4032-9f87-6234eaf99a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.30.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12bbddfa340>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'\n",
    "packages = [f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}', 'org.apache.kafka:kafka-clients:3.7.0']\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"kafka-example\")\\\n",
    "                    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"16g\")\\\n",
    "                    .config(\"spark.executor.cores\", \"8\")\\\n",
    "                    .config(\"spark.sql.shuffle.partitions\", \"1000\")\\\n",
    "                    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "                    .config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "624ea7a3-9c23-43a3-b843-ef2234eb69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'RandomImage'\n",
    "kafka_server = 'localhost:9092'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f67d475e-a4bc-445c-b641-e4fb9785f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from time import sleep\n",
    "from kafka import KafkaProducer\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import segmentation_models_pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35e45c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOAD_PATH = f'./models/supreme/abhi_sudo_full_2-pretrained_preproc_1-FPN+-ep_9-0.01-24.pt'\n",
    "SAVE_PATH = f'./predictionsFPN'\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "LOAD_SIZE = 8 # pred generating load size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b591387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FPN ####\n",
    "ENCODER_DEPTH=5\n",
    "DPC=256\n",
    "DSC = 128\n",
    "BATCH_SIZE = [8]\n",
    "LR = [1e-2]\n",
    "EPOCHS= 50\n",
    "ENCODER_NAME= 'resnet34'\n",
    "\n",
    "\n",
    "CLASSES={'Background':0,'Building-flooded':1,'Building-non-flooded':2,'Road-flooded':3,'Road-non-flooded':4,\n",
    "         'Water':5,'Tree':6,'Vehicle':7,'Pool':8,'Grass':9}\n",
    "IMG_DIM= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad381b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60eba817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): FPNDecoder(\n",
       "    (p5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p4): FPNBlock(\n",
       "      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNBlock(\n",
       "      (skip_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNBlock(\n",
       "      (skip_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (seg_blocks): ModuleList(\n",
       "      (0): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeBlock()\n",
       "    (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(128, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = segmentation_models_pytorch.FPN(encoder_name=ENCODER_NAME, encoder_depth=ENCODER_DEPTH,\n",
    "                                              decoder_pyramid_channels=DPC, decoder_segmentation_channels=DSC, classes=len(CLASSES))\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL_LOAD_PATH,map_location=torch.device('cuda')))\n",
    "# model = model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf8875d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92568028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_mask(inp):\n",
    "    inp=inp.transpose((1, 2, 0))\n",
    "    t_mask=np.argmax(inp,axis=2).astype('float32')\n",
    "    t_mask=cv2.resize(t_mask, dsize=(4000, 3000))\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    t_mask = cv2.erode(t_mask, kernel, iterations=1)\n",
    "    return t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed22ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a54a2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing function\n",
    "def process_image(image_bytes, image_count):\n",
    "    try:\n",
    "        print(f\"Processing image {image_count}\")\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((IMG_DIM, IMG_DIM))\n",
    "        img = np.array(img)\n",
    "\n",
    "        transformed_img = val_transform(img)\n",
    "        inputs = transformed_img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs)\n",
    "            preds = torch.sigmoid(preds).cpu().numpy()\n",
    "\n",
    "        f_mask = reverse_transform_mask(preds[0])\n",
    "\n",
    "        # Convert mask to bytes\n",
    "        mask_bytes = io.BytesIO()\n",
    "        np.save(mask_bytes, f_mask)\n",
    "        mask_bytes.seek(0)\n",
    "\n",
    "        # Save the mask bytes to GridFS\n",
    "        mask_id = fs.put(mask_bytes, filename=f\"{image_count}_mask.npy\")\n",
    "\n",
    "        # Save the mask ID and image ID to MongoDB\n",
    "        image_doc = {\n",
    "            \"image_id\": image_count,\n",
    "            \"mask_gridfs_id\": mask_id\n",
    "        }\n",
    "        collection.insert_one(image_doc)\n",
    "        print(f\"Processed and saved mask for image {image_count} to MongoDB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_count}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbcf03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "view_created = False\n",
    "view_created_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "274c03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    print(f\"Foreach Batch Function called on Epoch ID: {epoch_id}.\")\n",
    "    if df.count() > 0:\n",
    "        print(f\"Processing {df.count()} records in the batch.\")\n",
    "        pandas_df = df.toPandas()\n",
    "        for index, row in pandas_df.iterrows():\n",
    "            print(f\"Processing row {index}\")\n",
    "            process_image(row['image_bytes'], row['key'])\n",
    "    else:\n",
    "        print(\"Empty DataFrame received in foreachBatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4da5aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoint directory\n",
    "import shutil\n",
    "shutil.rmtree(\"checkpoint_dir\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "112c1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from Kafka stream\n",
    "streamRawDf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).option(\"startingOffsets\", \"latest\").load()\n",
    "streamDF = streamRawDf.select(col(\"key\").cast(\"string\"), col(\"value\").alias(\"image_bytes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b50e9001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreach Batch Function called on Epoch ID: 0.\n",
      "Empty DataFrame received in foreachBatch\n",
      "Foreach Batch Function called on Epoch ID: 1.\n",
      "Processing 1 records in the batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing image 7333\n",
      "Processed and saved mask for image 7333 to MongoDB\n",
      "Foreach Batch Function called on Epoch ID: 2.\n",
      "Processing 1 records in the batch.\n",
      "Processing row 0\n",
      "Processing image 7413\n",
      "Processed and saved mask for image 7413 to MongoDB\n",
      "Foreach Batch Function called on Epoch ID: 3.\n",
      "Processing 1 records in the batch.\n",
      "Processing row 0\n",
      "Processing image 7415\n",
      "Processed and saved mask for image 7415 to MongoDB\n",
      "Foreach Batch Function called on Epoch ID: 4.\n",
      "Processing 1 records in the batch.\n",
      "Processing row 0\n",
      "Processing image 7420\n",
      "Processed and saved mask for image 7420 to MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Start the streaming query\n",
    "query = streamDF.writeStream.foreachBatch(foreach_batch_function).option(\"checkpointLocation\", \"checkpoint_dir\").start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
