{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import logging\n",
    "from kafka import KafkaProducer\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, struct, udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "import segmentation_models_pytorch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pymongo import MongoClient\n",
    "import gridfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging (Driver-Side)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "python_version = 'D:/Anaconda/envs/cuda/python.exe'\n",
    "os.environ['PYSPARK_PYTHON'] = python_version\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Spark session on driver.\n"
     ]
    }
   ],
   "source": [
    "# Kafka and Spark configuration (Driver-Side)\n",
    "kafka_server = 'localhost:9092'\n",
    "topic_name = 'RandomImage'\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}', \n",
    "    'org.apache.kafka:kafka-clients:3.7.0',\n",
    "    'org.mongodb.spark:mongo-spark-connector_2.12:2.4.1'  # Use an older, stable version\n",
    "]\n",
    "\n",
    "# Spark session setup (Driver-Side)\n",
    "logger.info(\"Initializing Spark session on driver.\")\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName(\"kafka-example\") \\\n",
    "                    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.executor.cores\", \"8\") \\\n",
    "                    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model and transformation setup (Driver-Side)\n",
    "MODEL_LOAD_PATH = './models/abhi_sudo_full_2-pretrained_preproc_1-unet+-ep_13-0.01-24.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENCODER_DEPTH = 5\n",
    "DECODER_CHANNELS = (256, 128, 64, 32, 16)\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-2\n",
    "EPOCHS = 50\n",
    "ENCODER_NAME = 'resnet34'\n",
    "CLASSES = {'Background': 0, 'Building-flooded': 1, 'Building-non-flooded': 2, 'Road-flooded': 3, 'Road-non-flooded': 4,\n",
    "           'Water': 5, 'Tree': 6, 'Vehicle': 7, 'Pool': 8, 'Grass': 9}\n",
    "IMG_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = segmentation_models_pytorch.Unet(encoder_name=ENCODER_NAME, encoder_depth=ENCODER_DEPTH,\n",
    "                                         decoder_channels=DECODER_CHANNELS, classes=len(CLASSES))\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL_LOAD_PATH, map_location=torch.device('cuda')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Broadcast the model and transformation needed on the workers (Driver-Side)\n",
    "broadcast_model = spark.sparkContext.broadcast(model)\n",
    "broadcast_val_transform = spark.sparkContext.broadcast(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cuda\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Functions for image transformation and mask processing (Worker-Side)\n",
    "def reverse_transform_mask(inp):\n",
    "    inp = inp.transpose((1, 2, 0))\n",
    "    t_mask = np.argmax(inp, axis=2).astype('float32')\n",
    "    t_mask = cv2.resize(t_mask, dsize=(4000, 3000))\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    t_mask = cv2.erode(t_mask, kernel, iterations=1)\n",
    "    return t_mask\n",
    "\n",
    "# Pandas UDF to transform images (Worker-Side)\n",
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR)\n",
    "def transform_image_pandas_udf(image_bytes_series: pd.Series) -> pd.Series:\n",
    "    logger.info(\"Starting image transformation with Pandas UDF\")\n",
    "    def transform_image_single(image_bytes):\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((IMG_DIM, IMG_DIM))\n",
    "        img = np.array(img)\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        img = cv2.bilateralFilter(img, 5, 75, 75)\n",
    "        img = cv2.erode(cv2.dilate(img, kernel, iterations=2), kernel, iterations=1)\n",
    "        transformed_img = broadcast_val_transform.value(img)\n",
    "        return transformed_img.numpy().flatten().astype(np.float32)\n",
    "    \n",
    "    return image_bytes_series.apply(transform_image_single)\n",
    "\n",
    "# Predict function for batch processing (Worker-Side)\n",
    "def predict_batch_fn():\n",
    "    model = broadcast_model.value\n",
    "    model.eval()\n",
    "    def predict(inputs: np.ndarray):\n",
    "        inputs = inputs.reshape(-1, 3, IMG_DIM, IMG_DIM)\n",
    "        inputs_tensor = torch.from_numpy(inputs).to(device).float()\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs_tensor)\n",
    "            preds = torch.sigmoid(preds).cpu().numpy()\n",
    "        return preds.reshape(inputs.shape[0], -1)\n",
    "    return predict\n",
    "\n",
    "image_predict_udf = predict_batch_udf(\n",
    "    predict_batch_fn,\n",
    "    return_type=ArrayType(FloatType()),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_tensor_shapes=[[3 * IMG_DIM * IMG_DIM]]\n",
    ")\n",
    "\n",
    "# UDF for transforming the mask and saving to MongoDB (Worker-Side)\n",
    "def save_to_mongo(key, predictions):\n",
    "    logger.info(f\"Saving to MongoDB for key: {key}\")\n",
    "    predictions = np.array(predictions).reshape(len(CLASSES), IMG_DIM, IMG_DIM)\n",
    "    f_mask = reverse_transform_mask(predictions)\n",
    "    \n",
    "    # Convert mask to bytes and save to GridFS (Worker-Side)\n",
    "    mask_bytes = io.BytesIO()\n",
    "    np.save(mask_bytes, f_mask)\n",
    "    mask_bytes.seek(0)\n",
    "\n",
    "    # Setup MongoDB connection inside the worker function (Worker-Side)\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['image_db']\n",
    "    fs = gridfs.GridFS(db)\n",
    "    collection = db['image_masks']\n",
    "\n",
    "    mask_id = fs.put(mask_bytes, filename=f\"{key}_mask.npy\")\n",
    "\n",
    "    # Save to MongoDB (Worker-Side)\n",
    "    image_doc = {\n",
    "        \"image_id\": key,\n",
    "        \"mask_gridfs_id\": mask_id\n",
    "    }\n",
    "    collection.insert_one(image_doc)\n",
    "\n",
    "    logger.info(f\"Successfully saved to MongoDB for key: {key}\")\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_to_mongo_udf = udf(save_to_mongo, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Batch processing function (Driver-Side)\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    logger.info(f\"Foreach Batch Function called on Epoch ID: {epoch_id}.\")\n",
    "    if df.count() > 0:\n",
    "        logger.info(f\"Processing {df.count()} records in the batch.\")\n",
    "\n",
    "        # Transform the image (Worker-Side)\n",
    "        df = df.withColumn(\"transformed_image_bytes\", transform_image_pandas_udf(col(\"image_bytes\")))\n",
    "\n",
    "        # Make predictions (Worker-Side)\n",
    "        df = df.withColumn(\"predictions\", image_predict_udf(\"transformed_image_bytes\"))\n",
    "\n",
    "        # Save predictions to MongoDB (Worker-Side)\n",
    "        df = df.withColumn(\"saved_to_mongo\", save_to_mongo_udf(col(\"key\"), col(\"predictions\")))\n",
    "\n",
    "        df.show()\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Empty DataFrame received in foreachBatch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean the checkpoint directory (Driver-Side)\n",
    "import shutil\n",
    "shutil.rmtree(\"checkpoint_dir\", ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting up Kafka stream on driver.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame from Kafka stream (Driver-Side)\n",
    "logger.info(\"Setting up Kafka stream on driver.\")\n",
    "streamRawDf = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "streamDF = streamRawDf.select(col(\"key\").cast(\"string\"), col(\"value\").alias(\"image_bytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting the streaming query on driver.\n",
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 53097)\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 0.\n",
      "INFO:__main__:Empty DataFrame received in foreachBatch\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 1.\n",
      "INFO:__main__:Processing 1 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|7413|[FF D8 FF E0 00 1...|   [-1.2616663, -1.2...|[0.066137396, 0.0...|          7413|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 2.\n",
      "INFO:__main__:Processing 3 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|7415|[FF D8 FF E0 00 1...|   [-1.1246684, -1.1...|[0.055927064, 0.0...|          7415|\n",
      "|7420|[FF D8 FF E0 00 1...|   [-0.5253019, -0.5...|[0.067052126, 0.0...|          7420|\n",
      "|7423|[FF D8 FF E0 00 1...|   [-1.3815396, -1.3...|[0.08092851, 0.05...|          7423|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 3.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|7431|[FF D8 FF E0 00 1...|   [-0.5253019, -0.5...|[0.08983604, 0.05...|          7431|\n",
      "|7450|[FF D8 FF E0 00 1...|   [0.31381115, 0.31...|[0.12111328, 0.09...|          7450|\n",
      "|7457|[FF D8 FF E0 00 1...|   [-1.0219197, -1.0...|[0.047142897, 0.0...|          7457|\n",
      "|7461|[FF D8 FF E0 00 1...|   [0.15968838, 0.15...|[0.08039708, 0.04...|          7461|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 4.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|7464|[FF D8 FF E0 00 1...|   [0.60493195, 0.60...|[0.06619186, 0.03...|          7464|\n",
      "|7476|[FF D8 FF E0 00 1...|   [-1.6041614, -1.6...|[0.057231933, 0.0...|          7476|\n",
      "|7486|[FF D8 FF E0 00 1...|   [0.15968838, 0.15...|[0.07117865, 0.04...|          7486|\n",
      "|7541|[FF D8 FF E0 00 1...|   [-1.0219197, -1.0...|[0.06371592, 0.03...|          7541|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 5.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "| key|         image_bytes|transformed_image_bytes|         predictions|saved_to_mongo|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "|7543|[FF D8 FF E0 00 1...|   [-0.5424266, -0.5...|[0.05732034, 0.02...|          7543|\n",
      "|7577|[FF D8 FF E0 00 1...|   [-0.38830382, -0....|[0.086897716, 0.0...|          7577|\n",
      "|7581|[FF D8 FF E0 00 1...|   [-0.6622999, -0.6...|[0.04989769, 0.02...|          7581|\n",
      "|7583|[FF D8 FF E0 00 1...|   [1.0844251, 1.084...|[0.072445035, 0.0...|          7583|\n",
      "+----+--------------------+-----------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:__main__:Foreach Batch Function called on Epoch ID: 6.\n",
      "INFO:__main__:Processing 4 records in the batch.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start the streaming query (Driver-Side)\n",
    "logger.info(\"Starting the streaming query on driver.\")\n",
    "query = streamDF.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query (Driver-Side)\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
